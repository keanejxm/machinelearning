>>> from nlpia import data
>>> sms = data.get_data('sms-spam')
>>> >>> from sklearn.decomposition import TruncatedSVD
... >>> from seaborn import plt
... >>> svd = TruncatedSVD(16)
... >>> svd = svd.fit(tfidf_docs)
... >>> svd_topicvectors = svd.transform(tfidf_docs)
... >>> svd_topicvectors = pd.DataFrame(svd_topicvectors,
...                                     columns=['topic{}'.format(i) for i in range(16)])
... >>> svd_topicvectors.head()
...
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(pca_topic_vectors)
...
>>> >>> from nlpia.data import get_data
... >>> from sklearn.feature_extraction.text import TfidfVectorizer
... >>> from sklearn.decomposition import LatentDirichletAllocation as LDiA
... >>> sms = get_data('sms-spam')
...
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(pca_topic_vectors)
...
>>> from nltk.tokenize import casual_tokenize
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(pca_topic_vectors)
...
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... tfidf_cov = tfidf_docs * tfidf_docs.T
... 
... from sklearn.decomposition import TruncatedSVD
... from seaborn import plt                                       
... svd = TruncatedSVD(16)                                             
... svd = svd.fit(tfidf_cov)                                              
... svd_topic_vectors = svd.transform(tfidf_cov)     
... svd_topic_vectors = pd.DataFrame(svd_topic_vectors,                                                               
...                                  columns=['topic{}'.format(i) for i in range(16)])
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(pca_topic_vectors)
...
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... tfidf_cov = tfidf_docs.dot(tfidf_docs.T)
... 
... from sklearn.decomposition import TruncatedSVD
... from seaborn import plt                                       
... svd = TruncatedSVD(16)                                             
... svd = svd.fit(tfidf_cov)                                              
... svd_topic_vectors = svd.transform(tfidf_cov)     
... svd_topic_vectors = pd.DataFrame(svd_topic_vectors,                                                               
...                                  columns=['topic{}'.format(i) for i in range(16)])
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(pca_topic_vectors)
...
>>> from nlpia.data import get_data
... sms = get_data('sms-spam')
... 
... from sklearn.feature_extraction.text import TfidfVectorizer
... from nltk.tokenize import casual_tokenize
... tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... tfidf_cov = tfidf_docs.dot(tfidf_docs.T)
... 
... from sklearn.decomposition import TruncatedSVD
... from seaborn import plt                                       
... svd = TruncatedSVD(16)                                             
... svd = svd.fit(tfidf_cov)                                              
... svd_topic_vectors = svd.transform(tfidf_cov)     
... 
... import pandas as pd 
... svd_topic_vectors = pd.DataFrame(svd_topic_vectors,                                                               
...                                  columns=['topic{}'.format(i) for i in range(16)])
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(pca_topic_vectors)
...
>>> from nlpia.data import get_data
... sms = get_data('sms-spam')
... 
... from sklearn.feature_extraction.text import TfidfVectorizer
... from nltk.tokenize import casual_tokenize
... tfidf = TfidfVectorizer(tokenizer=casual_tokenize)
... tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()
... tfidf_cov = tfidf_docs.dot(tfidf_docs.T)
... 
... from sklearn.decomposition import TruncatedSVD
... from seaborn import plt                                       
... svd = TruncatedSVD(16)                                             
... svd = svd.fit(tfidf_cov)                                              
... svd_topic_vectors = svd.transform(tfidf_cov)     
... 
... import pandas as pd 
... svd_topic_vectors = pd.DataFrame(svd_topic_vectors,                                                               
...                                  columns=['topic{}'.format(i) for i in range(16)])
... 
... from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
... from sklearn.cross_validation import train_test_split
... X_train, X_test, y_train, y_test = train_test_split(svd_topic_vectors, sms.spam, test_size=0.5, random_state=271828)
... lda = LDA(n_components=1)
... lda = lda.fit(X_train, y_train)
... sms['svd16_spam'] = lda.predict(svd_topic_vectors)
...
>>> round(float(lda.score(X_test, y_test)), 3)
0.965
>>> hist -o -p
>>> hist
>>> svd_topic_vectors
        topic0    topic1    topic2    topic3    topic4    topic5    topic6  \
0     1.489844  1.466645  0.008454  0.299272  0.178251  0.135156 -0.318024   
1     1.412461  3.129878 -0.582580  0.008079  0.032591 -0.557215  0.294317   
2     1.476877 -0.350724 -0.346677  0.119319  0.606952 -0.432649 -0.259448   
3     1.734098  2.549076 -0.198588 -0.021652  0.715931 -0.061424  0.193065   
4     1.500242 -0.013773  0.190177  0.310782 -0.161569  0.394019 -0.554183   
5     2.901691  0.021502  0.380083  0.141317  0.474031 -0.617519 -0.240235   
6     2.577861 -0.340306 -0.631901 -0.334668  0.031440  0.218648  0.178690   
7     1.270177 -0.298353 -0.162110 -0.005860  0.201367 -0.331381 -0.249096   
8     2.891278 -0.956846 -0.616795  0.145036  0.630320 -1.499108 -0.269323   
9     2.295563 -0.330783  0.217766  0.297099  1.038233 -0.697181 -0.154136   
10    2.935130 -0.073343  0.299827  0.077434 -0.127380  0.591368 -0.167019   
11    2.020770 -0.462206 -0.200219  0.445455  0.680661 -0.656035 -0.532253   
12    2.143345 -0.589313 -0.064712  0.216918  0.973121 -1.297946 -0.658719   
13    3.733538 -0.713552 -0.293463 -0.467922 -0.514907 -0.076830 -0.403522   
14    2.705270 -0.391512  0.039503 -0.031467  0.540161 -0.630573 -1.374046   
15    1.749640 -0.253273  0.110249  0.142011  0.332381 -0.101035 -0.082432   
16    1.510125  1.627192  0.110841 -0.024645 -0.320083  0.337978 -0.170184   
17    2.414899  0.736802 -0.417102 -0.372319  0.051606  0.259130  0.082837   
18    1.859315 -0.208933 -0.258769 -0.185674  0.408083  0.395412 -0.030334   
19    1.426924 -0.326235 -0.378429  0.113971  0.426081 -0.300953 -0.186962   
20    1.785374 -0.126894  1.197156 -0.383019 -0.051683 -0.150407  0.446758   
21    1.241790  0.015879 -0.160719 -0.106888  0.168575  0.118791 -0.310599   
22    1.654839  2.317178 -0.129201  0.003018 -0.086907 -0.125673 -0.229964   
23    2.547765  0.904914 -0.449783 -0.400260  0.401475  0.575215  0.606357   
24    2.675259 -0.100904  0.486709 -0.385632 -0.316969  0.212025  0.297330   
25    3.015945 -0.525338 -1.093302 -0.385368 -0.067973  0.272394  0.230053   
26    1.498615 -0.289118 -0.474805 -0.239966 -0.028411 -0.023209  0.147281   
27    3.577239 -0.211096  2.679214 -0.674068 -0.499846 -0.672173  1.055613   
28    2.161202 -0.184981  0.329626  1.034672 -0.341066  0.162763 -0.069304   
29    2.821225 -0.295553  0.047556 -0.481799  0.036856  0.200173  0.172417   
...        ...       ...       ...       ...       ...       ...       ...   
4807  0.895919 -0.182760  0.075884 -0.027749  0.119219 -0.180773 -0.231067   
4808  1.367200  0.424869  0.699795 -0.108414  0.832955  0.169805  0.546373   
4809  2.624285 -0.218545  0.230811  0.723192 -0.113203  0.265844 -0.369622   
4810  2.100605 -0.422264  0.706383 -0.216678 -0.397269 -0.328192  0.178913   
4811  1.804790 -0.375976 -0.504688 -0.357451 -0.118602 -0.193480  0.532369   
4812  2.856010 -0.643231 -0.292936  0.442127  0.650758 -0.573509 -0.093767   
4813  1.920833  0.008991  0.067853  0.350071 -0.093574  0.568492 -0.595966   
4814  2.781092 -0.384457  0.126660  1.099072  0.041567  0.166741 -0.028775   
4815  3.431360 -0.155832  2.093020  0.094574 -0.676961 -0.218234  0.593830   
4816  1.993695 -0.383829  0.123181 -0.318220 -0.381268 -0.125425  0.091862   
4817  2.260687 -0.557290 -0.087012 -0.285292 -0.147338 -0.427084 -0.254398   
4818  0.654861  0.018502  0.151335 -0.079263  0.430341  0.467906 -0.171561   
4819  1.169123 -0.117577 -0.045393  0.061993  0.141138  0.009772 -0.320465   
4820  2.780639 -0.268645 -0.867440 -0.227750  0.421566  0.096651  0.246682   
4821  3.162462  0.627211 -0.348475 -0.463885 -0.271279 -0.040606  0.144645   
4822  4.497339 -1.089945 -0.501258 -0.728976 -0.460880 -0.051480  0.309583   
4823  1.982564 -0.402582 -0.062045  5.599621 -0.883155  0.435423  1.199086   
4824  1.622921 -0.206263  0.296330 -0.073367 -0.363906  0.209781 -0.225992   
4825  1.886001  0.367722 -1.124666 -0.387271 -0.046743  0.039952  0.650505   
4826  2.832552 -0.496918 -0.767878 -0.359211  0.031031  0.116063  0.082943   
4827  2.081470  2.908607 -0.651042 -0.136208 -0.444523 -0.158449 -0.193189   
4828  1.271149  0.298728 -0.743498 -0.276647  0.013326  0.051584  0.561473   
4829  2.929108 -0.557945  0.333659 -0.270076 -0.630826 -0.337570 -0.020275   
4830  0.676734  1.746656 -0.084213  0.049929 -0.153970 -0.374620 -0.260900   
4831  2.204845 -0.467090 -0.150815  0.522763  0.475857 -0.427136 -0.345405   
4832  3.878788 -0.731908 -1.072287  0.279633  1.658106 -0.672619  0.413304   
4833  1.554455  0.480844  0.505604 -0.220668  0.263178  0.242105  0.236544   
4834  2.623569  0.742495  0.294250  0.262259 -0.034827 -0.082783 -0.061350   
4835  2.137430 -0.171937  0.042593 -0.062369 -0.054728  0.354946 -0.599392   
4836  1.411734 -0.349245 -0.517362 -0.260215  0.097658  0.034978  0.055872   

        topic7    topic8    topic9   topic10   topic11   topic12   topic13  \
0     0.360200 -0.401475 -0.349928 -0.088644 -0.013419  0.034075  0.024994   
1     0.183833  0.233395  0.078872 -0.157892 -0.079929 -0.161498  0.199742   
2     0.052520  0.107193 -0.133155  0.683485 -0.203349 -0.107562  0.141706   
3    -0.643244 -0.507288 -0.166794 -0.335295 -0.384108 -0.126575  0.344869   
4    -0.232112  0.405052  0.031471  0.099119 -0.208659  0.007070  0.139012   
5     0.246984  0.812688  0.236906  0.320241  0.072816 -0.229668 -0.205269   
6    -0.065017 -0.077301 -0.331065  0.016918 -0.516271 -0.127452 -0.410138   
7     0.130634 -0.029000  0.040637  0.226005  0.067718  0.608237 -0.104267   
8     0.212538 -0.310183  0.641661 -0.043017  0.020790 -0.212323  0.194403   
9    -0.105466  0.108039  0.212528  0.783033  0.295397  0.302225 -0.344786   
10   -0.112021  0.107890 -0.236153  0.572963  0.209709 -0.224311 -0.049533   
11    0.080602  0.302146 -0.044799  0.595438 -0.332349  0.064946  0.083432   
12    0.333756 -0.016662  0.347674  0.139728  0.168635 -0.296900  0.044895   
13   -0.049696  0.002317 -0.047778  0.037437  0.116506  0.364405  0.050013   
14   -0.091625  0.115534  1.055916 -0.635724  0.713011 -0.403836 -0.027095   
15    0.016427  0.216899 -0.306465  0.308935  0.317099  0.201094 -0.139560   
16    0.904366 -1.273384 -0.464754  0.469643  1.241032 -0.907088  0.718335   
17   -0.530407 -0.284374  0.235149 -0.357803  0.026046  0.160203  0.263800   
18   -0.144767  0.321588 -0.396886 -0.285635  0.257792 -0.241090 -0.061928   
19    0.040233  0.151597 -0.125018  0.370802 -0.127936  0.098282  0.056063   
20    0.107957  0.180526 -0.233297 -0.167391  0.226360  0.284951  0.127095   
21   -0.224809 -0.008665  0.246659  0.406052  0.016645  0.088912  0.059513   
22   -0.064449 -0.602453 -0.513286  0.232823  0.159969  0.186122 -0.033699   
23   -0.738418 -0.152003  0.396139 -0.136447  0.044383  0.315370  0.018979   
24   -0.221274  0.092693  0.104692  0.104627 -0.027467  0.120678 -0.104790   
25   -0.111231 -0.200926 -0.515917 -0.059388  0.129239 -0.076254 -0.254592   
26   -0.059309 -0.181026  0.008545 -0.059100  0.144961  0.437033 -0.083631   
27    0.121628  0.009032 -0.068709 -0.195634  0.185945  0.398257 -0.186741   
28    0.060331  0.045660 -0.484355  0.043501  0.021434 -0.481672 -0.461678   
29   -0.272537  0.187177  0.276999 -0.351325  0.315073  0.255710 -0.036129   
...        ...       ...       ...       ...       ...       ...       ...   
4807 -0.022174  0.151058  0.026935  0.605952 -0.134696  0.121433 -0.036348   
4808 -0.443329  0.042682 -0.100052 -0.165132  0.109889 -0.069237  0.261771   
4809  0.098385  0.288661 -0.866566  0.302541  0.079147 -0.444233 -0.821489   
4810  0.403023 -0.101905 -0.281490 -0.158918  0.204711 -0.417499 -0.295731   
4811  0.095020 -0.156048 -0.267751 -0.257755  0.024829  0.026101  0.136856   
4812  0.161815  0.177844 -0.017257  0.385729  0.181162  0.614130 -0.244460   
4813 -0.112704  0.380767  0.078814 -0.311010  0.080178  0.330396  0.188462   
4814  0.003720  0.475362 -0.678235 -0.483703 -0.190504  0.186701  0.147382   
4815 -0.007181  0.397507 -0.257940  0.116498 -0.553284  0.011708  0.030923   
4816  0.016870  0.007153 -0.189138 -0.224782 -0.234845 -0.099316  0.033555   
4817  0.159556  0.102560 -0.242175  0.232629 -0.367475 -0.192783  0.296112   
4818  1.101511 -0.401613  0.340465  0.024883 -0.320616  0.583178 -0.189258   
4819  0.042434  0.043728 -0.245143 -0.071141  0.200911 -0.054788  0.144866   
4820 -0.240628 -0.146662 -0.377216 -0.328294 -0.073427 -0.085771  0.277316   
4821 -0.402293 -0.450015  0.095826 -0.515257 -0.238574 -0.011844  0.370191   
4822  0.183448  0.042862 -0.616594 -0.318483  0.202042  0.183381  0.099369   
4823  0.038895 -0.370182  0.436656 -0.028082  0.137412  0.200194  0.257744   
4824  0.108504  0.111730 -0.353601 -0.146317  0.199511 -0.328085 -0.197138   
4825 -0.113316 -0.149567 -0.226997 -0.223334  0.027157  0.213239  0.125131   
4826 -0.132248 -0.003984 -0.223597  0.518687 -0.516703 -0.055698 -0.139909   
4827 -0.193637  0.028074  0.258786 -0.056947  0.004129  0.203764  0.220598   
4828 -0.156217 -0.087816 -0.120582 -0.142859  0.016021  0.089904  0.013148   
4829  0.112210  0.023461 -0.018965  0.374193 -0.472975 -0.182464 -0.161057   
4830 -0.046848 -0.454882 -0.275104  0.010221  0.027535  0.195884  0.097378   
4831  0.138343  0.142538 -0.099105  0.490142 -0.230265  0.258050 -0.025464   
4832 -0.268224 -0.195581 -0.056350 -0.450318  0.301221 -0.073419  0.507995   
4833 -0.211096 -0.132257 -0.012205  1.026240  0.483287 -0.041224 -0.391256   
4834 -0.109813 -0.013514 -0.460309 -0.276547  0.338725  0.445141 -0.026422   
4835 -0.156376  0.392557 -0.274121  0.359057  0.183826 -0.134735 -0.010727   
4836  0.132159  0.073956 -0.233151  0.209627 -0.164862 -0.012604  0.166702   

       topic14   topic15  
0    -0.158998 -0.200192  
1    -0.098821  0.311171  
2    -0.053932 -0.292814  
3    -0.162520  0.332027  
4     0.242935 -0.124559  
5     0.298337 -0.099210  
6     0.349595  0.173236  
7     0.343756  0.227243  
8    -0.282955 -0.358090  
9    -0.181784  0.718553  
10    0.600995 -0.264953  
11    0.372872 -0.238735  
12   -0.430098 -0.224922  
13   -0.072808  0.209132  
14   -0.242982 -0.349556  
15   -0.102855  0.219421  
16    1.214062  0.291427  
17   -0.003938  0.127875  
18   -0.370353  0.683619  
19    0.100317 -0.000924  
20   -0.131223  0.224661  
21   -0.072083 -0.103971  
22   -0.200454 -0.009634  
23    0.130873 -0.478908  
24   -0.085898  0.038846  
25    0.162377 -0.261351  
26    0.229720  0.252954  
27   -0.164784 -0.106188  
28    0.038992  0.134619  
29    0.078560 -0.009608  
...        ...       ...  
4807  0.143925  0.115855  
4808 -0.202482 -0.239726  
4809  0.136738  0.193232  
4810 -0.470391  0.053433  
4811 -0.101890 -0.392018  
4812  0.694545  0.138059  
4813  0.199738 -0.137261  
4814  0.685895 -0.097462  
4815  0.148772  0.141167  
4816  0.046324  0.132777  
4817 -0.206979 -0.272083  
4818 -0.122507  0.106475  
4819 -0.251836 -0.212627  
4820  0.073395 -0.448183  
4821 -0.152145  0.278695  
4822 -0.449019  0.472352  
4823 -0.628949  0.145698  
4824 -0.250166  0.086794  
4825  0.041709 -0.261902  
4826  0.301412 -0.194632  
4827 -0.157975  0.026032  
4828  0.063817 -0.220803  
4829  0.190115  0.105258  
4830 -0.053868  0.047918  
4831  0.203221  0.296573  
4832 -0.579029  0.502378  
4833 -0.055891 -0.835699  
4834  0.222991  0.087748  
4835 -0.200966  0.201505  
4836 -0.034213 -0.082996  

[4837 rows x 16 columns]
>>> %paste
>>> >>> svd = TruncatedSVD(16)  # <1>
... >>> svd = svd.fit(tfidf_cov)
... >>> svd_topic_vectors = svd.transform(tfidf_cov)
... >>> svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                      columns=['topic{}'.format(i) for i in range(16)])
...
>>> svd_topic_vectors.head()
     topic0    topic1    topic2    topic3    topic4    topic5    topic6  \
0  1.489844  1.466645  0.008455  0.299272  0.178258  0.135160 -0.318040   
1  1.412461  3.129878 -0.582579  0.008080  0.032591 -0.557219  0.294314   
2  1.476877 -0.350724 -0.346677  0.119320  0.606949 -0.432650 -0.259441   
3  1.734098  2.549076 -0.198586 -0.021649  0.715924 -0.061419  0.193087   
4  1.500242 -0.013773  0.190179  0.310781 -0.161560  0.394019 -0.554190   

     topic7    topic8    topic9   topic10   topic11   topic12   topic13  \
0  0.360130 -0.401583 -0.350144 -0.087677 -0.013971  0.038736  0.015871   
1  0.183844  0.233293  0.078755 -0.158066 -0.077779 -0.158768  0.192125   
2  0.052590  0.107111 -0.132707  0.682431 -0.195155 -0.101948  0.126464   
3 -0.643187 -0.507165 -0.165962 -0.336004 -0.380877 -0.123150  0.337950   
4 -0.232167  0.405218  0.031766  0.100489 -0.209756  0.006349  0.140268   

    topic14   topic15  
0 -0.163910  0.199520  
1 -0.104264 -0.326426  
2 -0.053187  0.268240  
3 -0.163733 -0.327687  
4  0.244101  0.146831  
>>> tfidf_cov
array([[ 1.        ,  0.07648041,  0.00751233, ...,  0.05819219,
         0.01110585,  0.        ],
       [ 0.07648041,  1.        ,  0.        , ...,  0.05754573,
         0.        ,  0.        ],
       [ 0.00751233,  0.        ,  1.        , ...,  0.01524479,
         0.03746244,  0.02958896],
       ..., 
       [ 0.05819219,  0.05754573,  0.01524479, ...,  1.        ,
         0.03624248,  0.01070738],
       [ 0.01110585,  0.        ,  0.03746244, ...,  0.03624248,
         1.        ,  0.01204931],
       [ 0.        ,  0.        ,  0.02958896, ...,  0.01070738,
         0.01204931,  1.        ]])
>>> pd.DataFrame(tfidf_cov, columns=['doc{}'.format(i) for i in range(len(tfidf_cov))]
>>> columns=['doc{}'.format(i) for i in range(len(tfidf_cov))]
>>> pd.DataFrame(tfidf_cov, columns=columns, index=index)
>>> pd.DataFrame(tfidf_cov, columns=columns, index=columns)
             doc0      doc1      doc2      doc3      doc4      doc5      doc6  \
doc0     1.000000  0.076480  0.007512  0.061247  0.009975  0.028187  0.000000   
doc1     0.076480  1.000000  0.000000  0.164680  0.000000  0.030705  0.000000   
doc2     0.007512  0.000000  1.000000  0.000000  0.017192  0.061457  0.022794   
doc3     0.061247  0.164680  0.000000  1.000000  0.000000  0.000000  0.000000   
doc4     0.009975  0.000000  0.017192  0.000000  1.000000  0.023567  0.010223   
doc5     0.028187  0.030705  0.061457  0.000000  0.023567  1.000000  0.062704   
doc6     0.000000  0.000000  0.022794  0.000000  0.010223  0.062704  1.000000   
doc7     0.000000  0.000000  0.077890  0.000000  0.005820  0.033791  0.012283   
doc8     0.019482  0.000000  0.034638  0.000000  0.014305  0.108023  0.038608   
doc9     0.000000  0.016731  0.050381  0.026797  0.014497  0.051073  0.033445   
doc10    0.008840  0.000000  0.018438  0.000000  0.077547  0.040905  0.020482   
doc11    0.012191  0.000000  0.095750  0.000000  0.036895  0.078232  0.026617   
doc12    0.009448  0.000000  0.085494  0.039446  0.007207  0.110009  0.006728   
doc13    0.000000  0.000000  0.021015  0.000000  0.019384  0.099619  0.041277   
doc14    0.000000  0.000000  0.011979  0.000000  0.016956  0.071890  0.046145   
doc15    0.037972  0.000000  0.033958  0.000000  0.047899  0.020392  0.014257   
doc16    0.050332  0.091497  0.000000  0.073272  0.091790  0.000000  0.000000   
doc17    0.074148  0.063071  0.018417  0.066869  0.096639  0.000000  0.020832   
doc18    0.000000  0.020884  0.054707  0.033448  0.000000  0.000000  0.010648   
doc19    0.004945  0.000000  0.032462  0.000000  0.017679  0.018857  0.022913   
doc20    0.000000  0.000000  0.000000  0.000000  0.000000  0.023989  0.026783   
doc21    0.028235  0.162054  0.031008  0.000000  0.020830  0.028179  0.009678   
doc22    0.065290  0.238964  0.000000  0.176092  0.000000  0.000000  0.021779   
doc23    0.020734  0.019484  0.005569  0.110818  0.017040  0.036777  0.036445   
doc24    0.000000  0.000000  0.004300  0.000000  0.013157  0.073389  0.051147   
doc25    0.000000  0.000000  0.027788  0.000000  0.077092  0.009339  0.078077   
doc26    0.000000  0.000000  0.006087  0.051675  0.000000  0.000000  0.021716   
doc27    0.000000  0.000000  0.026515  0.000000  0.000000  0.055346  0.013727   
doc28    0.010596  0.000000  0.021461  0.000000  0.014620  0.069983  0.000000   
doc29    0.000000  0.000000  0.007578  0.000000  0.011595  0.079583  0.105126   
...           ...       ...       ...       ...       ...       ...       ...   
doc4807  0.000000  0.000000  0.064255  0.000000  0.013817  0.018260  0.012898   
doc4808  0.038075  0.032986  0.010191  0.052832  0.000000  0.052814  0.000000   
doc4809  0.006959  0.000000  0.042650  0.000000  0.017243  0.052681  0.040956   
doc4810  0.020668  0.000000  0.014923  0.000000  0.000000  0.055915  0.000000   
doc4811  0.037057  0.000000  0.017047  0.046930  0.000000  0.033786  0.029227   
doc4812  0.013042  0.000000  0.076206  0.000000  0.025153  0.066843  0.031960   
doc4813  0.014156  0.000000  0.000000  0.000000  0.035218  0.066788  0.000000   
doc4814  0.035099  0.000000  0.025912  0.000000  0.048426  0.037529  0.031767   
doc4815  0.015764  0.000000  0.000000  0.000000  0.185680  0.054195  0.000000   
doc4816  0.049415  0.000000  0.004431  0.037619  0.000000  0.025706  0.015809   
doc4817  0.000000  0.000000  0.042309  0.000000  0.020229  0.046091  0.058574   
doc4818  0.029815  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   
doc4819  0.059262  0.000000  0.014682  0.000000  0.000000  0.012355  0.000000   
doc4820  0.028375  0.000000  0.038299  0.000000  0.102811  0.008755  0.055447   
doc4821  0.030693  0.082528  0.007640  0.152359  0.011689  0.008307  0.027258   
doc4822  0.000000  0.000000  0.012578  0.000000  0.005499  0.028869  0.064141   
doc4823  0.023218  0.000000  0.000000  0.000000  0.032033  0.021168  0.000000   
doc4824  0.015418  0.000000  0.011132  0.000000  0.056849  0.008436  0.023100   
doc4825  0.000000  0.000000  0.009605  0.000000  0.000000  0.000000  0.034268   
doc4826  0.000000  0.000000  0.041248  0.000000  0.019977  0.026402  0.110647   
doc4827  0.069851  0.164256  0.012485  0.261771  0.017735  0.018696  0.000000   
doc4828  0.000000  0.000000  0.006142  0.000000  0.000000  0.044800  0.148494   
doc4829  0.000000  0.000000  0.060625  0.000000  0.070658  0.034020  0.026050   
doc4830  0.054482  0.099040  0.000000  0.079313  0.000000  0.000000  0.000000   
doc4831  0.036465  0.000000  0.056127  0.000000  0.017931  0.051614  0.029244   
doc4832  0.026703  0.033917  0.030576  0.054323  0.009233  0.050281  0.060566   
doc4833  0.000000  0.000000  0.024012  0.000000  0.015294  0.033465  0.014278   
doc4834  0.058192  0.057546  0.015245  0.079532  0.015010  0.036949  0.014057   
doc4835  0.011106  0.000000  0.037462  0.000000  0.092427  0.149718  0.065503   
doc4836  0.000000  0.000000  0.029589  0.000000  0.015575  0.020584  0.032869   

             doc7      doc8      doc9    ...      doc4827   doc4828   doc4829  \
doc0     0.000000  0.019482  0.000000    ...     0.069851  0.000000  0.000000   
doc1     0.000000  0.000000  0.016731    ...     0.164256  0.000000  0.000000   
doc2     0.077890  0.034638  0.050381    ...     0.012485  0.006142  0.060625   
doc3     0.000000  0.000000  0.026797    ...     0.261771  0.000000  0.000000   
doc4     0.005820  0.014305  0.014497    ...     0.017735  0.000000  0.070658   
doc5     0.033791  0.108023  0.051073    ...     0.018696  0.044800  0.034020   
doc6     0.012283  0.038608  0.033445    ...     0.000000  0.148494  0.026050   
doc7     1.000000  0.065209  0.041240    ...     0.000000  0.006238  0.129447   
doc8     0.065209  1.000000  0.052877    ...     0.000000  0.022998  0.048872   
doc9     0.041240  0.052877  1.000000    ...     0.000000  0.000000  0.034291   
doc10    0.008409  0.024665  0.021271    ...     0.015717  0.010401  0.069515   
doc11    0.012912  0.055721  0.080187    ...     0.000000  0.007172  0.027383   
doc12    0.003831  0.165309  0.074185    ...     0.000000  0.000000  0.025759   
doc13    0.097584  0.095964  0.057670    ...     0.020799  0.020646  0.088443   
doc14    0.000000  0.147095  0.096068    ...     0.027205  0.000000  0.000000   
doc15    0.029521  0.017169  0.064192    ...     0.036765  0.007241  0.029145   
doc16    0.000000  0.000000  0.000000    ...     0.083566  0.000000  0.000000   
doc17    0.005930  0.021863  0.010263    ...     0.086895  0.018972  0.012576   
doc18    0.003031  0.011175  0.046793    ...     0.000000  0.009697  0.036230   
doc19    0.009408  0.027593  0.025264    ...     0.000000  0.011636  0.019952   
doc20    0.000000  0.010739  0.013146    ...     0.000000  0.000000  0.037065   
doc21    0.017154  0.038987  0.102114    ...     0.039374  0.000000  0.011685   
doc22    0.000000  0.000000  0.000000    ...     0.170019  0.000000  0.000000   
doc23    0.005656  0.020851  0.017114    ...     0.110691  0.267614  0.011994   
doc24    0.004367  0.025341  0.036701    ...     0.021110  0.013970  0.041157   
doc25    0.015592  0.059160  0.009355    ...     0.011444  0.037868  0.033068   
doc26    0.070827  0.022791  0.026836    ...     0.039289  0.019777  0.058809   
doc27    0.024485  0.030997  0.054516    ...     0.000000  0.000000  0.102822   
doc28    0.000000  0.008248  0.023339    ...     0.000000  0.000000  0.066787   
doc29    0.007696  0.064819  0.022280    ...     0.018603  0.095737  0.016322   
...           ...       ...       ...    ...          ...       ...       ...   
doc4807  0.055746  0.018049  0.038384    ...     0.000000  0.000000  0.094538   
doc4808  0.000000  0.012719  0.028974    ...     0.000000  0.000000  0.000000   
doc4809  0.004061  0.015398  0.053209    ...     0.000000  0.000000  0.052476   
doc4810  0.000000  0.011178  0.025666    ...     0.000000  0.000000  0.038580   
doc4811  0.008320  0.041726  0.000000    ...     0.035681  0.026617  0.017645   
doc4812  0.057157  0.058593  0.243602    ...     0.000000  0.023019  0.041059   
doc4813  0.028294  0.000000  0.000000    ...     0.025169  0.000000  0.000000   
doc4814  0.019195  0.024502  0.022701    ...     0.000000  0.016520  0.054043   
doc4815  0.000000  0.024540  0.015020    ...     0.000000  0.000000  0.140748   
doc4816  0.004500  0.026116  0.000000    ...     0.028602  0.014397  0.085922   
doc4817  0.014140  0.070481  0.046480    ...     0.000000  0.010840  0.054738   
doc4818  0.055279  0.000000  0.022947    ...     0.085871  0.000000  0.039078   
doc4819  0.009937  0.029588  0.041844    ...     0.000000  0.000000  0.000000   
doc4820  0.009675  0.051091  0.000000    ...     0.025167  0.030951  0.020518   
doc4821  0.007759  0.054437  0.013429    ...     0.119031  0.024824  0.119506   
doc4822  0.031859  0.075260  0.050751    ...     0.008822  0.040869  0.133591   
doc4823  0.000000  0.038483  0.038998    ...     0.000000  0.000000  0.000000   
doc4824  0.000000  0.074856  0.023596    ...     0.000000  0.000000  0.067524   
doc4825  0.009755  0.035964  0.000000    ...     0.101931  0.247249  0.020688   
doc4826  0.020657  0.063108  0.026446    ...     0.000000  0.032116  0.076158   
doc4827  0.000000  0.000000  0.000000    ...     1.000000  0.065181  0.000000   
doc4828  0.006238  0.022998  0.000000    ...     0.065181  1.000000  0.013229   
doc4829  0.129447  0.048872  0.034291    ...     0.000000  0.013229  1.000000   
doc4830  0.000000  0.000000  0.000000    ...     0.090455  0.000000  0.000000   
doc4831  0.032058  0.064379  0.088002    ...     0.017310  0.000000  0.054367   
doc4832  0.034300  0.200460  0.065631    ...     0.032012  0.031496  0.020879   
doc4833  0.008129  0.019979  0.033521    ...     0.000000  0.000000  0.017239   
doc4834  0.045668  0.014752  0.027075    ...     0.077988  0.012801  0.008486   
doc4835  0.014020  0.011067  0.080899    ...     0.013721  0.052450  0.009549   
doc4836  0.013495  0.039582  0.020618    ...     0.000000  0.016692  0.028620   

          doc4830   doc4831   doc4832   doc4833   doc4834   doc4835   doc4836  
doc0     0.054482  0.036465  0.026703  0.000000  0.058192  0.011106  0.000000  
doc1     0.099040  0.000000  0.033917  0.000000  0.057546  0.000000  0.000000  
doc2     0.000000  0.056127  0.030576  0.024012  0.015245  0.037462  0.029589  
doc3     0.079313  0.000000  0.054323  0.000000  0.079532  0.000000  0.000000  
doc4     0.000000  0.017931  0.009233  0.015294  0.015010  0.092427  0.015575  
doc5     0.000000  0.051614  0.050281  0.033465  0.036949  0.149718  0.020584  
doc6     0.000000  0.029244  0.060566  0.014278  0.014057  0.065503  0.032869  
doc7     0.000000  0.032058  0.034300  0.008129  0.045668  0.014020  0.013495  
doc8     0.000000  0.064379  0.200460  0.019979  0.014752  0.011067  0.039582  
doc9     0.000000  0.088002  0.065631  0.033521  0.027075  0.080899  0.020618  
doc10    0.000000  0.063296  0.044856  0.089003  0.033855  0.050889  0.022503  
doc11    0.000000  0.126198  0.029873  0.028038  0.022946  0.024904  0.034551  
doc12    0.000000  0.078435  0.169025  0.010066  0.014217  0.072955  0.010250  
doc13    0.000000  0.063130  0.079115  0.035177  0.057650  0.072011  0.026402  
doc14    0.000000  0.035864  0.088994  0.068561  0.000000  0.013118  0.000000  
doc15    0.000000  0.086974  0.044598  0.021807  0.036893  0.062622  0.015664  
doc16    0.065179  0.000000  0.000000  0.000000  0.037871  0.000000  0.000000  
doc17    0.030376  0.062088  0.082998  0.000000  0.029819  0.077650  0.103387  
doc18    0.000000  0.000000  0.073367  0.073956  0.055642  0.021215  0.105551  
doc19    0.000000  0.027412  0.033981  0.015164  0.014905  0.014764  0.025174  
doc20    0.000000  0.076413  0.038495  0.027739  0.063034  0.000000  0.149781  
doc21    0.000000  0.027948  0.063910  0.074618  0.023897  0.033066  0.014744  
doc22    0.084549  0.000000  0.031303  0.078952  0.084782  0.000000  0.000000  
doc23    0.000000  0.000000  0.048398  0.015458  0.023678  0.013183  0.015134  
doc24    0.000000  0.055756  0.022049  0.023871  0.027603  0.010179  0.011685  
doc25    0.000000  0.021079  0.092475  0.009869  0.024291  0.035314  0.041724  
doc26    0.000000  0.029417  0.031213  0.000000  0.066103  0.000000  0.016542  
doc27    0.000000  0.011142  0.014212  0.063998  0.049979  0.060876  0.000000  
doc28    0.000000  0.010608  0.052819  0.000000  0.015946  0.013639  0.000000  
doc29    0.000000  0.000000  0.051339  0.021036  0.063597  0.061713  0.020595  
...           ...       ...       ...       ...       ...       ...       ...  
doc4807  0.000000  0.060855  0.000000  0.019297  0.000000  0.010689  0.019650  
doc4808  0.000000  0.000000  0.033593  0.026171  0.020438  0.000000  0.000000  
doc4809  0.000000  0.055543  0.057173  0.010671  0.010473  0.034529  0.010867  
doc4810  0.000000  0.035591  0.032909  0.000000  0.031102  0.022061  0.169802  
doc4811  0.000000  0.000000  0.042009  0.000000  0.017074  0.000000  0.022264  
doc4812  0.000000  0.124112  0.056179  0.023110  0.061134  0.037411  0.029436  
doc4813  0.000000  0.047467  0.013103  0.000000  0.086815  0.039717  0.000000  
doc4814  0.000000  0.035137  0.109848  0.000000  0.063416  0.000000  0.013818  
doc4815  0.000000  0.051861  0.059119  0.031694  0.048474  0.000000  0.000000  
doc4816  0.000000  0.112765  0.022723  0.000000  0.048122  0.069442  0.012043  
doc4817  0.000000  0.062846  0.048721  0.028253  0.006954  0.029817  0.037838  
doc4818  0.000000  0.025155  0.000000  0.000000  0.000000  0.000000  0.000000  
doc4819  0.000000  0.014951  0.022304  0.000000  0.037844  0.026844  0.000000  
doc4820  0.000000  0.022761  0.102821  0.000000  0.044202  0.071787  0.025889  
doc4821  0.039747  0.000000  0.102597  0.000000  0.106066  0.009044  0.020764  
doc4822  0.000000  0.056170  0.170845  0.000000  0.041095  0.031045  0.092854  
doc4823  0.000000  0.065991  0.061019  0.000000  0.034939  0.000000  0.000000  
doc4824  0.000000  0.000000  0.023917  0.000000  0.023202  0.086137  0.000000  
doc4825  0.000000  0.024605  0.049254  0.000000  0.020019  0.069626  0.026103  
doc4826  0.000000  0.074748  0.071537  0.027901  0.020601  0.015455  0.055275  
doc4827  0.090455  0.017310  0.032012  0.000000  0.077988  0.013721  0.000000  
doc4828  0.000000  0.000000  0.031496  0.000000  0.012801  0.052450  0.016692  
doc4829  0.000000  0.054367  0.020879  0.017239  0.008486  0.009549  0.028620  
doc4830  1.000000  0.000000  0.000000  0.000000  0.040993  0.000000  0.000000  
doc4831  0.000000  1.000000  0.064558  0.011097  0.010891  0.050786  0.089869  
doc4832  0.000000  0.064558  1.000000  0.000000  0.030274  0.017228  0.026345  
doc4833  0.000000  0.011097  0.000000  1.000000  0.021873  0.011833  0.021752  
doc4834  0.040993  0.010891  0.030274  0.021873  1.000000  0.036242  0.010707  
doc4835  0.000000  0.050786  0.017228  0.011833  0.036242  1.000000  0.012049  
doc4836  0.000000  0.089869  0.026345  0.021752  0.010707  0.012049  1.000000  

[4837 rows x 4837 columns]
>>> pd.DataFrame(tfidf_cov, columns=columns, index=columns).head()
          doc0     doc1      doc2      doc3      doc4      doc5      doc6  \
doc0  1.000000  0.07648  0.007512  0.061247  0.009975  0.028187  0.000000   
doc1  0.076480  1.00000  0.000000  0.164680  0.000000  0.030705  0.000000   
doc2  0.007512  0.00000  1.000000  0.000000  0.017192  0.061457  0.022794   
doc3  0.061247  0.16468  0.000000  1.000000  0.000000  0.000000  0.000000   
doc4  0.009975  0.00000  0.017192  0.000000  1.000000  0.023567  0.010223   

         doc7      doc8      doc9    ...      doc4827   doc4828   doc4829  \
doc0  0.00000  0.019482  0.000000    ...     0.069851  0.000000  0.000000   
doc1  0.00000  0.000000  0.016731    ...     0.164256  0.000000  0.000000   
doc2  0.07789  0.034638  0.050381    ...     0.012485  0.006142  0.060625   
doc3  0.00000  0.000000  0.026797    ...     0.261771  0.000000  0.000000   
doc4  0.00582  0.014305  0.014497    ...     0.017735  0.000000  0.070658   

       doc4830   doc4831   doc4832   doc4833   doc4834   doc4835   doc4836  
doc0  0.054482  0.036465  0.026703  0.000000  0.058192  0.011106  0.000000  
doc1  0.099040  0.000000  0.033917  0.000000  0.057546  0.000000  0.000000  
doc2  0.000000  0.056127  0.030576  0.024012  0.015245  0.037462  0.029589  
doc3  0.079313  0.000000  0.054323  0.000000  0.079532  0.000000  0.000000  
doc4  0.000000  0.017931  0.009233  0.015294  0.015010  0.092427  0.015575  

[5 rows x 4837 columns]
>>> hist -o -p
>>> >>> headings = ['doc{}'.format(i) for i in range(len(tfidf_cov))]
... >>> pd.DataFrame(tfidf_cov, columns=headings, index=headings).round(3).head()
...
       doc0   doc1   doc2   doc3   doc4   doc5   doc6   doc7   doc8   doc9  \
doc0  1.000  0.076  0.008  0.061  0.010  0.028  0.000  0.000  0.019  0.000   
doc1  0.076  1.000  0.000  0.165  0.000  0.031  0.000  0.000  0.000  0.017   
doc2  0.008  0.000  1.000  0.000  0.017  0.061  0.023  0.078  0.035  0.050   
doc3  0.061  0.165  0.000  1.000  0.000  0.000  0.000  0.000  0.000  0.027   
doc4  0.010  0.000  0.017  0.000  1.000  0.024  0.010  0.006  0.014  0.014   

       ...     doc4827  doc4828  doc4829  doc4830  doc4831  doc4832  doc4833  \
doc0   ...       0.070    0.000    0.000    0.054    0.036    0.027    0.000   
doc1   ...       0.164    0.000    0.000    0.099    0.000    0.034    0.000   
doc2   ...       0.012    0.006    0.061    0.000    0.056    0.031    0.024   
doc3   ...       0.262    0.000    0.000    0.079    0.000    0.054    0.000   
doc4   ...       0.018    0.000    0.071    0.000    0.018    0.009    0.015   

      doc4834  doc4835  doc4836  
doc0    0.058    0.011    0.000  
doc1    0.058    0.000    0.000  
doc2    0.015    0.037    0.030  
doc3    0.080    0.000    0.000  
doc4    0.015    0.092    0.016  

[5 rows x 4837 columns]
>>> >>> svd = TruncatedSVD(16)  # <1>
... >>> svd = svd.fit(tfidf_cov)
... >>> svd_topic_vectors = svd.transform(tfidf_cov)
... >>> svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                      columns=['topic{}'.format(i) for i in range(16)])
... >>> svd_topic_vectors.head()
...
     topic0    topic1    topic2    topic3    topic4    topic5    topic6  \
0  1.489844  1.466645  0.008454  0.299273  0.178250  0.135155 -0.318033   
1  1.412461  3.129878 -0.582580  0.008079  0.032594 -0.557217  0.294314   
2  1.476877 -0.350724 -0.346678  0.119319  0.606955 -0.432650 -0.259437   
3  1.734098  2.549076 -0.198587 -0.021651  0.715934 -0.061428  0.193086   
4  1.500242 -0.013773  0.190178  0.310782 -0.161566  0.394007 -0.554191   

     topic7    topic8    topic9   topic10   topic11   topic12   topic13  \
0  0.360189 -0.401622 -0.350452 -0.087223 -0.013270  0.035215  0.020801   
1  0.183814  0.233403  0.079033 -0.158037 -0.078555 -0.159841  0.193717   
2  0.052495  0.107087 -0.132351  0.683121 -0.199472 -0.113695  0.122649   
3 -0.643223 -0.507211 -0.165999 -0.336322 -0.382696 -0.128747  0.349912   
4 -0.232101  0.405074  0.031381  0.099247 -0.206574  0.003581  0.137688   

    topic14   topic15  
0 -0.164938 -0.192810  
1 -0.098685  0.312123  
2 -0.057786 -0.290834  
3 -0.160730  0.317978  
4  0.235360 -0.123694  
>>> >>> svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                      columns=['topic{}'.format(i) for i in range(16)],
...                                      index=['doc{}'.format(i) for i in range(len(tfidf_cov))])
... >>> svd_topic_vectors.head()
...
      topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  \
doc0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   
doc1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   
doc2     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   
doc3     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   
doc4     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   

      topic9  topic10  topic11  topic12  topic13  topic14  topic15  
doc0     NaN      NaN      NaN      NaN      NaN      NaN      NaN  
doc1     NaN      NaN      NaN      NaN      NaN      NaN      NaN  
doc2     NaN      NaN      NaN      NaN      NaN      NaN      NaN  
doc3     NaN      NaN      NaN      NaN      NaN      NaN      NaN  
doc4     NaN      NaN      NaN      NaN      NaN      NaN      NaN  
>>> >>> svd_topic_vectors = svd.transform(tfidf_cov)
... >>> svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                      columns=['topic{}'.format(i) for i in range(16)],
...                                      index=['doc{}'.format(i) for i in range(len(tfidf_cov))])
...
>>> >>> svd_topic_vectors.head()
        topic0    topic1    topic2    topic3    topic4    topic5    topic6  \
doc0  1.489844  1.466645  0.008454  0.299273  0.178250  0.135155 -0.318033   
doc1  1.412461  3.129878 -0.582580  0.008079  0.032594 -0.557217  0.294314   
doc2  1.476877 -0.350724 -0.346678  0.119319  0.606955 -0.432650 -0.259437   
doc3  1.734098  2.549076 -0.198587 -0.021651  0.715934 -0.061428  0.193086   
doc4  1.500242 -0.013773  0.190178  0.310782 -0.161566  0.394007 -0.554191   

        topic7    topic8    topic9   topic10   topic11   topic12   topic13  \
doc0  0.360189 -0.401622 -0.350452 -0.087223 -0.013270  0.035215  0.020801   
doc1  0.183814  0.233403  0.079033 -0.158037 -0.078555 -0.159841  0.193717   
doc2  0.052495  0.107087 -0.132351  0.683121 -0.199472 -0.113695  0.122649   
doc3 -0.643223 -0.507211 -0.165999 -0.336322 -0.382696 -0.128747  0.349912   
doc4 -0.232101  0.405074  0.031381  0.099247 -0.206574  0.003581  0.137688   

       topic14   topic15  
doc0 -0.164938 -0.192810  
doc1 -0.098685  0.312123  
doc2 -0.057786 -0.290834  
doc3 -0.160730  0.317978  
doc4  0.235360 -0.123694  
>>> svd.transform?
>>> >>> svd_topic_vectors = svd.fit_transform(tfidf_docs)
... >>> svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                      columns=['topic{}'.format(i) for i in range(16)],
...                                      index=['doc{}'.format(i) for i in range(len(tfidf_cov))])
... >>> svd_topic_vectors.round(3).head()
...
      topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  \
doc0   0.117   0.197   0.002   0.047   0.029   0.023  -0.055   0.065  -0.078   
doc1   0.111   0.421  -0.089   0.001   0.006  -0.096   0.052   0.034   0.048   
doc2   0.116  -0.047  -0.053   0.019   0.102  -0.074  -0.045   0.011   0.023   
doc3   0.136   0.343  -0.030  -0.003   0.120  -0.010   0.035  -0.120  -0.099   
doc4   0.118  -0.002   0.029   0.049  -0.027   0.068  -0.097  -0.044   0.079   

      topic9  topic10  topic11  topic12  topic13  topic14  topic15  
doc0  -0.069    0.020    0.001   -0.011    0.012   -0.009   -0.034  
doc1   0.013    0.034   -0.022    0.034    0.044    0.004    0.077  
doc2  -0.030   -0.139   -0.045    0.014    0.027    0.046   -0.043  
doc3  -0.030    0.069   -0.086    0.030    0.078   -0.010    0.057  
doc4   0.006   -0.023   -0.048    0.008   -0.031    0.054   -0.009  
>>> sms.spam[:5]
0    0
1    0
2    1
3    0
4    0
Name: spam, dtype: int64
>>> svd_topic_vectors.iloc([:3]).corr()
>>> svd_topic_vectors.iloc[:3].corr()
           topic0    topic1    topic2    topic3    topic4    topic5    topic6  \
topic0   1.000000 -0.760781  0.884261  0.878059  0.565865  0.753221 -0.997656   
topic1  -0.760781  1.000000 -0.369645 -0.357426 -0.965606 -0.146140  0.803406   
topic2   0.884261 -0.369645  1.000000  0.999914  0.115336  0.973217 -0.850234   
topic3   0.878059 -0.357426  0.999914  1.000000  0.102297  0.976149 -0.843256   
topic4   0.565865 -0.965606  0.115336  0.102297  1.000000 -0.116106 -0.620954   
topic5   0.753221 -0.146140  0.973217  0.976149 -0.116106  1.000000 -0.706449   
topic6  -0.997656  0.803406 -0.850234 -0.843256 -0.620954 -0.706449  1.000000   
topic7   0.244640  0.443171  0.669129  0.678819 -0.661012  0.822048 -0.177721   
topic8  -0.761314  0.158386 -0.975991 -0.978764  0.103790 -0.999923  0.715164   
topic9  -0.943656  0.503140 -0.988979 -0.986952 -0.261132 -0.928455  0.918801   
topic10 -0.426845  0.911650  0.044872  0.057972 -0.987151  0.273327  0.487722   
topic11  0.147963  0.529298  0.592691  0.603205 -0.731696  0.761976 -0.079945   
topic12 -0.900128  0.402076 -0.999382 -0.998835 -0.150179 -0.964536  0.868211   
topic13 -0.944612  0.505645 -0.988545 -0.986481 -0.263932 -0.927374  0.919942   
topic14  0.130220 -0.742552 -0.347869 -0.360137  0.891165 -0.554082 -0.197757   
topic15 -0.974896  0.886191 -0.758081 -0.749461 -0.735242 -0.587854  0.987847   

           topic7    topic8    topic9   topic10   topic11   topic12   topic13  \
topic0   0.244640 -0.761314 -0.943656 -0.426845  0.147963 -0.900128 -0.944612   
topic1   0.443171  0.158386  0.503140  0.911650  0.529298  0.402076  0.505645   
topic2   0.669129 -0.975991 -0.988979  0.044872  0.592691 -0.999382 -0.988545   
topic3   0.678819 -0.978764 -0.986952  0.057972  0.603205 -0.998835 -0.986481   
topic4  -0.661012  0.103790 -0.261132 -0.987151 -0.731696 -0.150179 -0.263932   
topic5   0.822048 -0.999923 -0.928455  0.273327  0.761976 -0.964536 -0.927374   
topic6  -0.177721  0.715164  0.918801  0.487722 -0.079945  0.868211  0.919942   
topic7   1.000000 -0.814929 -0.551728  0.772423  0.995139 -0.642595 -0.549306   
topic8  -0.814929  1.000000  0.932987 -0.261386 -0.753893  0.967732  0.931939   
topic9  -0.551728  0.932987  1.000000  0.103529 -0.466912  0.993572  0.999996   
topic10  0.772423 -0.261386  0.103529  1.000000  0.831214 -0.009731  0.106414   
topic11  0.995139 -0.753893 -0.466912  0.831214  1.000000 -0.564016 -0.464344   
topic12 -0.642595  0.967732  0.993572 -0.009731 -0.564016  1.000000  0.993239   
topic13 -0.549306  0.931939  0.999996  0.106414 -0.464344  0.993239  1.000000   
topic14 -0.929501  0.543724  0.205227 -0.952208 -0.961304  0.314701  0.202387   
topic15 -0.022604  0.597832  0.846282  0.617487  0.075961  0.780535  0.847824   

          topic14   topic15  
topic0   0.130220 -0.974896  
topic1  -0.742552  0.886191  
topic2  -0.347869 -0.758081  
topic3  -0.360137 -0.749461  
topic4   0.891165 -0.735242  
topic5  -0.554082 -0.587854  
topic6  -0.197757  0.987847  
topic7  -0.929501 -0.022604  
topic8   0.543724  0.597832  
topic9   0.205227  0.846282  
topic10 -0.952208  0.617487  
topic11 -0.961304  0.075961  
topic12  0.314701  0.780535  
topic13  0.202387  0.847824  
topic14  1.000000 -0.347716  
topic15 -0.347716  1.000000  
>>> svd_topic_vectors.iloc[:3].dot(svd_topic_vectors.iloc[:3])
>>> svd_topic_vectors.iloc[:3].dot(svd_topic_vectors.iloc[:3].T)
          doc0      doc1      doc2
doc0  0.076304  0.086752  0.008294
doc1  0.086752  0.223687 -0.000888
doc2  0.008294 -0.000888  0.064592
>>> svd_topic_vectors.iloc[:3].dot(svd_topic_vectors.iloc[:3].T).round(3)
       doc0   doc1   doc2
doc0  0.076  0.087  0.008
doc1  0.087  0.224 -0.001
doc2  0.008 -0.001  0.065
>>> svd_topic_vectors.iloc[:6].dot(svd_topic_vectors.iloc[:6].T).round(3)
       doc0   doc1   doc2   doc3   doc4   doc5
doc0  0.076  0.087  0.008  0.087  0.011  0.017
doc1  0.087  0.224 -0.001  0.169 -0.001  0.030
doc2  0.008 -0.001  0.065  0.004  0.018  0.053
doc3  0.087  0.169  0.004  0.200  0.004  0.002
doc4  0.011 -0.001  0.018  0.004  0.047  0.039
doc5  0.017  0.030  0.053  0.002  0.039  0.124
>>> sms.spam[:6]
0    0
1    0
2    1
3    0
4    0
5    1
Name: spam, dtype: int64
>>> hist -o -p
>>> svd_topic_vectors = svd.transform(tfidf_cov)
... n = len(sms)
... svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                  columns=['topic{}'.format(i) for i in range(16)],
...                                  index=['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(n), sms.spam)])
...
>>> svd_topic_vectors = svd.fit_transform(tfidf_docs)
... n = len(sms)
... svd_topic_vectors = pd.DataFrame(svd_topic_vectors,
...                                  columns=['topic{}'.format(i) for i in range(16)],
...                                  index=['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(n), sms.spam)])
...
>>> svd_topic_vectors.round(3).head()
       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  \
sms0    0.117   0.197   0.002   0.047   0.031   0.023  -0.056   0.066  -0.078   
sms1    0.111   0.421  -0.089   0.001   0.006  -0.097   0.050   0.032   0.045   
sms2!   0.116  -0.047  -0.053   0.019   0.103  -0.074  -0.045   0.009   0.022   
sms3    0.136   0.343  -0.031  -0.003   0.120  -0.011   0.034  -0.120  -0.101   
sms4    0.118  -0.002   0.029   0.048  -0.026   0.068  -0.098  -0.046   0.079   

       topic9  topic10  topic11  topic12  topic13  topic14  topic15  
sms0   -0.067   -0.017   -0.010    0.012   -0.036    0.008   -0.004  
sms1    0.012   -0.036    0.024    0.047    0.016   -0.033    0.014  
sms2!  -0.029    0.146    0.040    0.027   -0.014    0.001    0.036  
sms3   -0.038   -0.077    0.081    0.076    0.016   -0.071   -0.039  
sms4    0.007    0.022    0.035    0.029    0.020    0.075   -0.019  
>>> svd_topic_vectors.round(3).head(6)
       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  \
sms0    0.117   0.197   0.002   0.047   0.031   0.023  -0.056   0.066  -0.078   
sms1    0.111   0.421  -0.089   0.001   0.006  -0.097   0.050   0.032   0.045   
sms2!   0.116  -0.047  -0.053   0.019   0.103  -0.074  -0.045   0.009   0.022   
sms3    0.136   0.343  -0.031  -0.003   0.120  -0.011   0.034  -0.120  -0.101   
sms4    0.118  -0.002   0.029   0.048  -0.026   0.068  -0.098  -0.046   0.079   
sms5!   0.228   0.003   0.058   0.022   0.079  -0.106  -0.042   0.047   0.163   

       topic9  topic10  topic11  topic12  topic13  topic14  topic15  
sms0   -0.067   -0.017   -0.010    0.012   -0.036    0.008   -0.004  
sms1    0.012   -0.036    0.024    0.047    0.016   -0.033    0.014  
sms2!  -0.029    0.146    0.040    0.027   -0.014    0.001    0.036  
sms3   -0.038   -0.077    0.081    0.076    0.016   -0.071   -0.039  
sms4    0.007    0.022    0.035    0.029    0.020    0.075   -0.019  
sms5!   0.050    0.069   -0.005   -0.002   -0.050    0.097   -0.094  
>>> hist -o -p
>>> svd_topic_vectors.iloc[:6].dot(svd_topic_vectors.iloc[:6].T).round(3)
        sms0   sms1  sms2!   sms3   sms4  sms5!
sms0   0.076  0.089  0.008  0.088  0.012  0.020
sms1   0.089  0.218  0.002  0.167 -0.001  0.031
sms2!  0.008  0.002  0.063  0.003  0.015  0.052
sms3   0.088  0.167  0.003  0.204  0.004  0.006
sms4   0.012 -0.001  0.015  0.004  0.049  0.045
sms5!  0.020  0.031  0.052  0.006  0.045  0.132
>>> from numpy import linalg
>>> ans = linalg.svd(tfidf_cov)
>>> ans
(array([[ -9.21458686e-03,   2.65259171e-02,  -1.97061346e-04, ...,
           1.03280104e-19,  -3.02059255e-15,  -3.58351857e-15],
        [ -8.73597509e-03,   5.66073388e-02,   1.35796537e-02, ...,
          -7.86897830e-19,   2.83592411e-13,  -1.51304705e-13],
        [ -9.13438765e-03,  -6.34323820e-03,   8.08087897e-03, ...,
           3.49764106e-18,  -4.07497644e-02,  -5.20506631e-02],
        ..., 
        [ -1.62265965e-02,   1.34288482e-02,  -6.85883614e-03, ...,
           5.66361545e-19,   4.81819445e-16,  -5.23235968e-16],
        [ -1.32198606e-02,  -3.10967489e-03,  -9.92830079e-04, ...,
           2.43740701e-19,  -2.17013907e-15,   1.66164825e-15],
        [ -8.73147936e-03,  -6.31648888e-03,   1.20594583e-02, ...,
           1.49049484e-18,   5.37222176e-16,  -4.05925293e-16]]),
 array([  1.61683243e+02,   5.52910358e+01,   4.29009025e+01, ...,
          1.23169732e-14,   1.23169732e-14,   7.50024414e-15]),
 array([[ -9.21458686e-03,  -8.73597509e-03,  -9.13438765e-03, ...,
          -1.62265965e-02,  -1.32198606e-02,  -8.73147936e-03],
        [  2.65259171e-02,   5.66073388e-02,  -6.34323820e-03, ...,
           1.34288482e-02,  -3.10967489e-03,  -6.31648888e-03],
        [ -1.97061346e-04,   1.35796537e-02,   8.08087897e-03, ...,
          -6.85883614e-03,  -9.92830079e-04,   1.20594583e-02],
        ..., 
        [  0.00000000e+00,  -5.99376828e-19,   6.46379528e-18, ...,
          -2.66839644e-19,   1.70258920e-18,  -1.12694737e-19],
        [  0.00000000e+00,  -1.81978443e-13,   1.97033463e-02, ...,
           2.59688104e-15,   1.29349656e-14,  -6.77582990e-15],
        [  0.00000000e+00,   1.99825210e-13,  -5.72190766e-02, ...,
          -2.58300326e-15,   5.85816118e-15,  -3.76781939e-15]]))
>>> U, S, V = linalg.svd(tfidf_docs)
>>> U
array([[ -9.21458686e-03,   2.65259171e-02,  -1.97061346e-04, ...,
         -1.54208826e-16,   2.75571496e-16,   1.12821930e-16],
       [ -8.73597509e-03,   5.66073388e-02,   1.35796537e-02, ...,
         -2.33147836e-18,  -1.58740880e-16,   1.07596440e-17],
       [ -9.13438765e-03,  -6.34323820e-03,   8.08087897e-03, ...,
         -1.82123550e-02,   7.82344909e-03,   2.97617459e-02],
       ..., 
       [ -1.62265965e-02,   1.34288482e-02,  -6.85883614e-03, ...,
          2.81350464e-17,  -6.98226199e-17,  -3.21872520e-17],
       [ -1.32198606e-02,  -3.10967489e-03,  -9.92830079e-04, ...,
          6.64073831e-17,  -2.77555756e-17,  -7.04731412e-18],
       [ -8.73147936e-03,  -6.31648888e-03,   1.20594583e-02, ...,
         -2.05727362e-17,   1.26851654e-17,  -3.23363298e-17]])
>>> U.shape
(4837, 4837)
>>> len(sms)
4837
>>> tfidf_docs.shape
(4837, 9232)
>>> utopic_vectors = U.dot(tfidf_docs)
>>> U.shape
(4837, 4837)
>>> S.diag()
>>> S
array([  1.27154726e+01,   7.43579423e+00,   6.54987805e+00, ...,
         5.09818826e-16,   3.75416421e-16,   2.45240794e-16])
>>> S.round(3)
array([ 12.715,   7.436,   6.55 , ...,   0.   ,   0.   ,   0.   ])
>>> V.shape
(9232, 9232)
>>> tfidf_docs.shape
(4837, 9232)
>>> V[:16,:].dot(tfidf_docs.T)
array([[-0.11716783, -0.11108205, -0.11614806, ..., -0.20632884,
        -0.16809678, -0.11102489],
       [ 0.19724126,  0.42092052, -0.04716701, ...,  0.09985415,
        -0.0231229 , -0.04696811],
       [-0.00129073,  0.08894508,  0.05292877, ..., -0.04492454,
        -0.00650292,  0.07898798],
       ..., 
       [ 0.00475609,  0.04345764,  0.02872906, ..., -0.0051957 ,
        -0.00206403,  0.0382502 ],
       [-0.03567055, -0.02265387, -0.01283405, ...,  0.047922  ,
        -0.04582984, -0.0076032 ],
       [ 0.04503597, -0.07090406,  0.06784454, ..., -0.01699949,
        -0.04558498,  0.0189509 ]])
>>> vtopic_vectors = V[:16,:].dot(tfidf_docs.T)
>>> v_topic_vectors.iloc[:6].dot(v_topic_vectors.iloc[:6].T).round(3)
>>> v_topic_vectors = V[:16,:].dot(tfidf_docs.T)
>>> v_topic_vectors.iloc[:6].dot(v_topic_vectors.iloc[:6].T).round(3)
>>> v_topic_vectors[:6,:].dot(v_topic_vectors[:6,:].T).round(3)
array([[ 161.683,    0.   ,   -0.   ,   -0.   ,   -0.   ,   -0.   ],
       [   0.   ,   55.291,   -0.   ,   -0.   ,   -0.   ,    0.   ],
       [  -0.   ,   -0.   ,   42.901,   -0.   ,    0.   ,    0.   ],
       [  -0.   ,   -0.   ,   -0.   ,   40.652,    0.   ,   -0.   ],
       [  -0.   ,   -0.   ,    0.   ,    0.   ,   35.545,    0.   ],
       [  -0.   ,    0.   ,    0.   ,   -0.   ,    0.   ,   33.776]])
>>> sim = v_topic_vectors[:6,:].dot(v_topic_vectors[:6,:].T)
>>> sim /= sim.diag()
>>> sim /= np.diag(sim)
>>> sim /= pd.np.diag(sim)
>>> sim
array([[  1.00000000e+00,   1.58340359e-14,  -4.41880171e-16,
         -5.53032427e-16,  -6.18435046e-16,  -1.73186672e-15],
       [  5.41478653e-15,   1.00000000e+00,  -1.08043907e-16,
         -6.56213940e-16,  -2.49872746e-17,   4.84429624e-16],
       [ -1.17248132e-16,  -8.38324162e-17,   1.00000000e+00,
         -2.86211350e-15,   3.88474034e-16,   3.86434318e-16],
       [ -1.39049761e-16,  -4.82475635e-16,  -2.71209616e-15,
          1.00000000e+00,   5.51672172e-16,  -4.12525311e-16],
       [ -1.35959766e-16,  -1.60636965e-17,   3.21867328e-16,
          4.82367173e-16,   1.00000000e+00,   2.98197455e-16],
       [ -3.61786878e-16,   2.95923409e-16,   3.04237409e-16,
         -3.42743554e-16,   2.83351789e-16,   1.00000000e+00]])
>>> sim.round(3)
array([[ 1.,  0., -0., -0., -0., -0.],
       [ 0.,  1., -0., -0., -0.,  0.],
       [-0., -0.,  1., -0.,  0.,  0.],
       [-0., -0., -0.,  1.,  0., -0.],
       [-0., -0.,  0.,  0.,  1.,  0.],
       [-0.,  0.,  0., -0.,  0.,  1.]])
>>> sim.round(6)
array([[ 1.,  0., -0., -0., -0., -0.],
       [ 0.,  1., -0., -0., -0.,  0.],
       [-0., -0.,  1., -0.,  0.,  0.],
       [-0., -0., -0.,  1.,  0., -0.],
       [-0., -0.,  0.,  0.,  1.,  0.],
       [-0.,  0.,  0., -0.,  0.,  1.]])
>>> sim.round(9)
array([[ 1.,  0., -0., -0., -0., -0.],
       [ 0.,  1., -0., -0., -0.,  0.],
       [-0., -0.,  1., -0.,  0.,  0.],
       [-0., -0., -0.,  1.,  0., -0.],
       [-0., -0.,  0.,  0.,  1.,  0.],
       [-0.,  0.,  0., -0.,  0.,  1.]])
>>> (sim * 1e10).round(6)
array([[  1.00000000e+10,   1.58000000e-04,  -4.00000000e-06,
         -6.00000000e-06,  -6.00000000e-06,  -1.70000000e-05],
       [  5.40000000e-05,   1.00000000e+10,  -1.00000000e-06,
         -7.00000000e-06,  -0.00000000e+00,   5.00000000e-06],
       [ -1.00000000e-06,  -1.00000000e-06,   1.00000000e+10,
         -2.90000000e-05,   4.00000000e-06,   4.00000000e-06],
       [ -1.00000000e-06,  -5.00000000e-06,  -2.70000000e-05,
          1.00000000e+10,   6.00000000e-06,  -4.00000000e-06],
       [ -1.00000000e-06,  -0.00000000e+00,   3.00000000e-06,
          5.00000000e-06,   1.00000000e+10,   3.00000000e-06],
       [ -4.00000000e-06,   3.00000000e-06,   3.00000000e-06,
         -3.00000000e-06,   3.00000000e-06,   1.00000000e+10]])
>>> (sim * 1e15).round(6)
array([[  1.00000000e+15,   1.58340360e+01,  -4.41880000e-01,
         -5.53032000e-01,  -6.18435000e-01,  -1.73186700e+00],
       [  5.41478700e+00,   1.00000000e+15,  -1.08044000e-01,
         -6.56214000e-01,  -2.49870000e-02,   4.84430000e-01],
       [ -1.17248000e-01,  -8.38320000e-02,   1.00000000e+15,
         -2.86211400e+00,   3.88474000e-01,   3.86434000e-01],
       [ -1.39050000e-01,  -4.82476000e-01,  -2.71209600e+00,
          1.00000000e+15,   5.51672000e-01,  -4.12525000e-01],
       [ -1.35960000e-01,  -1.60640000e-02,   3.21867000e-01,
          4.82367000e-01,   1.00000000e+15,   2.98197000e-01],
       [ -3.61787000e-01,   2.95923000e-01,   3.04237000e-01,
         -3.42744000e-01,   2.83352000e-01,   1.00000000e+15]])
>>> sim
array([[  1.00000000e+00,   1.58340359e-14,  -4.41880171e-16,
         -5.53032427e-16,  -6.18435046e-16,  -1.73186672e-15],
       [  5.41478653e-15,   1.00000000e+00,  -1.08043907e-16,
         -6.56213940e-16,  -2.49872746e-17,   4.84429624e-16],
       [ -1.17248132e-16,  -8.38324162e-17,   1.00000000e+00,
         -2.86211350e-15,   3.88474034e-16,   3.86434318e-16],
       [ -1.39049761e-16,  -4.82475635e-16,  -2.71209616e-15,
          1.00000000e+00,   5.51672172e-16,  -4.12525311e-16],
       [ -1.35959766e-16,  -1.60636965e-17,   3.21867328e-16,
          4.82367173e-16,   1.00000000e+00,   2.98197455e-16],
       [ -3.61786878e-16,   2.95923409e-16,   3.04237409e-16,
         -3.42743554e-16,   2.83351789e-16,   1.00000000e+00]])
>>> V[:16,:].dot(tfidf_docs.T).round(3)
array([[-0.117, -0.111, -0.116, ..., -0.206, -0.168, -0.111],
       [ 0.197,  0.421, -0.047, ...,  0.1  , -0.023, -0.047],
       [-0.001,  0.089,  0.053, ..., -0.045, -0.007,  0.079],
       ..., 
       [ 0.005,  0.043,  0.029, ..., -0.005, -0.002,  0.038],
       [-0.036, -0.023, -0.013, ...,  0.048, -0.046, -0.008],
       [ 0.045, -0.071,  0.068, ..., -0.017, -0.046,  0.019]])
>>> V[:16,:].dot(tfidf_docs.T).round(2)
array([[-0.12, -0.11, -0.12, ..., -0.21, -0.17, -0.11],
       [ 0.2 ,  0.42, -0.05, ...,  0.1 , -0.02, -0.05],
       [-0.  ,  0.09,  0.05, ..., -0.04, -0.01,  0.08],
       ..., 
       [ 0.  ,  0.04,  0.03, ..., -0.01, -0.  ,  0.04],
       [-0.04, -0.02, -0.01, ...,  0.05, -0.05, -0.01],
       [ 0.05, -0.07,  0.07, ..., -0.02, -0.05,  0.02]])
>>> V[:16,:].dot(tfidf_docs.T).round(2)[:6,:6]
array([[-0.12, -0.11, -0.12, -0.14, -0.12, -0.23],
       [ 0.2 ,  0.42, -0.05,  0.34, -0.  ,  0.  ],
       [-0.  ,  0.09,  0.05,  0.03, -0.03, -0.06],
       [ 0.05,  0.  ,  0.02, -0.  ,  0.05,  0.02],
       [-0.03, -0.01, -0.1 , -0.12,  0.03, -0.08],
       [ 0.02, -0.1 , -0.07, -0.01,  0.07, -0.11]])
>>> V
array([[ -1.44207575e-01,  -2.84868500e-02,  -1.42764783e-03, ...,
         -4.32449982e-04,  -4.32449982e-04,  -4.32449982e-04],
       [ -5.15834887e-02,   8.74483571e-03,  -9.45556617e-04, ...,
          8.33850339e-04,   8.33850339e-04,   8.33850339e-04],
       [ -6.01357584e-02,  -6.22621537e-03,  -2.61131605e-04, ...,
         -1.07283816e-03,  -1.07283816e-03,  -1.07283816e-03],
       ..., 
       [  7.70684087e-19,  -4.14274840e-05,  -1.89675155e-04, ...,
          7.87697482e-01,  -2.12302518e-01,  -2.12302518e-01],
       [  7.63188005e-19,  -4.14274840e-05,  -1.89675155e-04, ...,
         -2.12302518e-01,   7.87697482e-01,  -2.12302518e-01],
       [  1.80817223e-18,  -4.14274840e-05,  -1.89675155e-04, ...,
         -2.12302518e-01,  -2.12302518e-01,   7.87697482e-01]])
>>> V.shape
(9232, 9232)
>>> S.shape
(4837,)
>>> U.shape
(4837, 4837)
>>> Vt = V[:,:16]
>>> Ut = U[:16,:]
>>> Vt.dot(tfidf_docs.T).round(2)[:6,:6]
>>> Vt.dot(tfidf_docs).round(2)[:6,:6]
>>> Vt.T.dot(tfidf_docs.T).round(2)[:6,:6]
array([[-0.02, -0.02, -0.08, -0.02, -0.03, -0.04],
       [-0.  ,  0.  ,  0.01,  0.01,  0.  , -0.01],
       [-0.01, -0.01, -0.02,  0.  ,  0.02, -0.01],
       [ 0.02,  0.01,  0.01, -0.17, -0.01,  0.01],
       [ 0.  , -0.  , -0.01, -0.01, -0.22,  0.01],
       [ 0.  ,  0.  ,  0.01,  0.01,  0.01, -0.01]])
>>> utopics = Ut.T.dot(tfidf_docs.T)
>>> utopics = Ut.dot(tfidf_docs.T)
>>> utopics = Ut.dot(tfidf_docs)
>>> utopics
array([[  4.58115026e-03,   7.05420548e-03,   1.67022309e-03, ...,
         -1.45381568e-17,  -1.45381568e-17,  -1.45381568e-17],
       [  1.99739407e-02,  -1.24173785e-02,  -1.15244187e-02, ...,
          4.85932805e-17,   4.85932805e-17,   4.85932805e-17],
       [  6.46411101e-02,   9.96769431e-02,  -2.35882553e-03, ...,
          1.05881344e-02,   1.05881344e-02,   1.05881344e-02],
       ..., 
       [  1.78407174e-02,  -8.08635259e-03,  -1.83677406e-03, ...,
          1.55481760e-17,   1.55481760e-17,   1.55481760e-17],
       [ -1.16828046e-01,  -2.06563839e-02,  -9.84254626e-03, ...,
         -3.24052178e-17,  -3.24052178e-17,  -3.24052178e-17],
       [ -2.34444621e-02,  -4.74247833e-02,  -9.00073578e-03, ...,
         -2.03377621e-17,  -2.03377621e-17,  -2.03377621e-17]])
>>> utopics.round(3)
array([[ 0.005,  0.007,  0.002, ..., -0.   , -0.   , -0.   ],
       [ 0.02 , -0.012, -0.012, ...,  0.   ,  0.   ,  0.   ],
       [ 0.065,  0.1  , -0.002, ...,  0.011,  0.011,  0.011],
       ..., 
       [ 0.018, -0.008, -0.002, ...,  0.   ,  0.   ,  0.   ],
       [-0.117, -0.021, -0.01 , ..., -0.   , -0.   , -0.   ],
       [-0.023, -0.047, -0.009, ..., -0.   , -0.   , -0.   ]])
>>> utopics.round(3).shape
(16, 9232)
>>> utopics.dot(utopics.T).T[:6]
array([[  1.13629094e+00,  -8.59316824e-03,  -1.73810379e-02,
         -6.03020018e-03,  -2.66798046e-03,   1.42508967e-02,
         -2.60621989e-02,   1.62704528e-02,  -4.41514813e-03,
         -3.78783046e-02,   2.21317501e-02,  -7.87091246e-02,
          2.30338074e-02,  -6.05392963e-03,  -1.07173687e-01,
          6.18509731e-02],
       [ -8.59316824e-03,   9.91797356e-01,  -3.40112486e-02,
         -1.67496284e-02,  -8.29128732e-03,  -5.60552533e-02,
         -3.80275029e-02,  -4.18617932e-02,  -2.35174659e-02,
         -1.62574485e-02,  -2.96539209e-02,  -6.37113469e-02,
          2.02579007e-02,   3.79451026e-02,  -3.96665120e-03,
          1.09957104e-02],
       [ -1.73810379e-02,  -3.40112486e-02,   1.04345863e+00,
         -3.04264393e-02,  -2.99126858e-02,   3.47505004e-02,
         -3.08347659e-02,   1.69851379e-02,   4.55439964e-04,
          1.14509663e-02,  -2.41388025e-02,   1.73429766e-02,
         -4.26339400e-03,   5.86165955e-02,   6.88356601e-02,
         -3.27777669e-02],
       [ -6.03020018e-03,  -1.67496284e-02,  -3.04264393e-02,
          1.02578354e+00,   1.14544902e-02,  -6.60693144e-02,
          4.36405930e-02,   2.33643788e-03,   6.31695878e-02,
         -6.20766274e-02,   7.55838859e-03,  -5.38682254e-02,
         -2.94074558e-02,  -1.55017345e-02,   4.00756675e-02,
          1.74211142e-03],
       [ -2.66798046e-03,  -8.29128732e-03,  -2.99126858e-02,
          1.14544902e-02,   9.62631349e-01,  -1.96432724e-02,
          2.78032701e-02,  -3.00643606e-02,  -4.69216285e-02,
         -3.94674975e-02,  -5.15823658e-03,  -1.33679704e-02,
          2.86989006e-02,  -3.23224829e-02,  -4.04627213e-02,
         -2.75205910e-02],
       [  1.42508967e-02,  -5.60552533e-02,   3.47505004e-02,
         -6.60693144e-02,  -1.96432724e-02,   9.58038938e-01,
          4.58593036e-03,   5.07108688e-02,  -7.36056834e-03,
          1.79912256e-02,   5.64293963e-02,   6.62198585e-02,
          2.21121794e-02,   3.10694379e-02,  -2.37694167e-03,
         -8.80567159e-03]])
>>> utopics.dot(utopics.T).T[:6].round(3)
array([[ 1.136, -0.009, -0.017, -0.006, -0.003,  0.014, -0.026,  0.016,
        -0.004, -0.038,  0.022, -0.079,  0.023, -0.006, -0.107,  0.062],
       [-0.009,  0.992, -0.034, -0.017, -0.008, -0.056, -0.038, -0.042,
        -0.024, -0.016, -0.03 , -0.064,  0.02 ,  0.038, -0.004,  0.011],
       [-0.017, -0.034,  1.043, -0.03 , -0.03 ,  0.035, -0.031,  0.017,
         0.   ,  0.011, -0.024,  0.017, -0.004,  0.059,  0.069, -0.033],
       [-0.006, -0.017, -0.03 ,  1.026,  0.011, -0.066,  0.044,  0.002,
         0.063, -0.062,  0.008, -0.054, -0.029, -0.016,  0.04 ,  0.002],
       [-0.003, -0.008, -0.03 ,  0.011,  0.963, -0.02 ,  0.028, -0.03 ,
        -0.047, -0.039, -0.005, -0.013,  0.029, -0.032, -0.04 , -0.028],
       [ 0.014, -0.056,  0.035, -0.066, -0.02 ,  0.958,  0.005,  0.051,
        -0.007,  0.018,  0.056,  0.066,  0.022,  0.031, -0.002, -0.009]])
>>> utopics.dot(utopics.T).shape
(16, 16)
>>> utopics.T.dot(utopics).shape
(9232, 9232)
>>> utopics.T.dot(utopics)[:6].round(3)
array([[ 0.097,  0.006,  0.003, ...,  0.001,  0.001,  0.001],
       [ 0.006,  0.03 ,  0.001, ...,  0.001,  0.001,  0.001],
       [ 0.003,  0.001,  0.001, ..., -0.   , -0.   , -0.   ],
       [ 0.001, -0.   ,  0.   , ..., -0.   , -0.   , -0.   ],
       [-0.002, -0.001,  0.   , ..., -0.   , -0.   , -0.   ],
       [ 0.   , -0.001, -0.001, ...,  0.   ,  0.   ,  0.   ]])
>>> utopics.T.dot(utopics)[:6].round(3).shape
(6, 9232)
>>> utopics
array([[  4.58115026e-03,   7.05420548e-03,   1.67022309e-03, ...,
         -1.45381568e-17,  -1.45381568e-17,  -1.45381568e-17],
       [  1.99739407e-02,  -1.24173785e-02,  -1.15244187e-02, ...,
          4.85932805e-17,   4.85932805e-17,   4.85932805e-17],
       [  6.46411101e-02,   9.96769431e-02,  -2.35882553e-03, ...,
          1.05881344e-02,   1.05881344e-02,   1.05881344e-02],
       ..., 
       [  1.78407174e-02,  -8.08635259e-03,  -1.83677406e-03, ...,
          1.55481760e-17,   1.55481760e-17,   1.55481760e-17],
       [ -1.16828046e-01,  -2.06563839e-02,  -9.84254626e-03, ...,
         -3.24052178e-17,  -3.24052178e-17,  -3.24052178e-17],
       [ -2.34444621e-02,  -4.74247833e-02,  -9.00073578e-03, ...,
         -2.03377621e-17,  -2.03377621e-17,  -2.03377621e-17]])
>>> utopics.shape
(16, 9232)
>>> utopics.T.dot(utopics).shape
(9232, 9232)
>>> utopics.T.dot(utopics)[:6,:6].round(3)
array([[ 0.097,  0.006,  0.003,  0.001, -0.002,  0.   ],
       [ 0.006,  0.03 ,  0.001, -0.   , -0.001, -0.001],
       [ 0.003,  0.001,  0.001,  0.   ,  0.   , -0.001],
       [ 0.001, -0.   ,  0.   ,  0.   , -0.   ,  0.   ],
       [-0.002, -0.001,  0.   , -0.   ,  0.001, -0.   ],
       [ 0.   , -0.001, -0.001,  0.   , -0.   ,  0.002]])
>>> utopics.T.dot(utopics)[:6,:6].round(3) * 100
array([[ 9.7,  0.6,  0.3,  0.1, -0.2,  0. ],
       [ 0.6,  3. ,  0.1, -0. , -0.1, -0.1],
       [ 0.3,  0.1,  0.1,  0. ,  0. , -0.1],
       [ 0.1, -0. ,  0. ,  0. , -0. ,  0. ],
       [-0.2, -0.1,  0. , -0. ,  0.1, -0. ],
       [ 0. , -0.1, -0.1,  0. , -0. ,  0.2]])
>>> C = utopics.T.dot(utopics)[:6,:6]
>>> C
array([[  9.73824650e-02,   6.37782027e-03,   2.92042288e-03,
          7.54399121e-04,  -1.52039975e-03,   2.70932870e-04],
       [  6.37782027e-03,   2.96141698e-02,   5.72640978e-04,
         -4.66368540e-04,  -9.11786385e-04,  -6.14833188e-04],
       [  2.92042288e-03,   5.72640978e-04,   1.16897244e-03,
          4.61214311e-06,   4.10290704e-04,  -5.20427142e-04],
       [  7.54399121e-04,  -4.66368540e-04,   4.61214311e-06,
          9.56493252e-05,  -6.80014256e-05,   7.23975232e-05],
       [ -1.52039975e-03,  -9.11786385e-04,   4.10290704e-04,
         -6.80014256e-05,   7.29008483e-04,  -3.21893245e-04],
       [  2.70932870e-04,  -6.14833188e-04,  -5.20427142e-04,
          7.23975232e-05,  -3.21893245e-04,   1.96098895e-03]])
>>> C.round(3)
array([[ 0.097,  0.006,  0.003,  0.001, -0.002,  0.   ],
       [ 0.006,  0.03 ,  0.001, -0.   , -0.001, -0.001],
       [ 0.003,  0.001,  0.001,  0.   ,  0.   , -0.001],
       [ 0.001, -0.   ,  0.   ,  0.   , -0.   ,  0.   ],
       [-0.002, -0.001,  0.   , -0.   ,  0.001, -0.   ],
       [ 0.   , -0.001, -0.001,  0.   , -0.   ,  0.002]])
>>> C2 = C / diag(C)
>>> C2 = C / pd.np.diag(C)
>>> C2
array([[  1.00000000e+00,   2.15363804e-01,   2.49828206e+00,
          7.88713480e+00,  -2.08557210e+00,   1.38161344e-01],
       [  6.54924916e-02,   1.00000000e+00,   4.89866962e-01,
         -4.87581631e+00,  -1.25072123e+00,  -3.13532204e-01],
       [  2.99892068e-02,   1.93367223e-02,   1.00000000e+00,
          4.82192958e-02,   5.62806488e-01,  -2.65390145e-01],
       [  7.74676551e-03,  -1.57481551e-02,   3.94546778e-03,
          1.00000000e+00,  -9.32793338e-02,   3.69188838e-02],
       [ -1.56126645e-02,  -3.07888551e-02,   3.50984069e-01,
         -7.10945168e-01,   1.00000000e+00,  -1.64148424e-01],
       [  2.78215251e-03,  -2.07614528e-02,  -4.45200523e-01,
          7.56905739e-01,  -4.41549382e-01,   1.00000000e+00]])
>>> C2.round(3)
array([[  1.00000000e+00,   2.15000000e-01,   2.49800000e+00,
          7.88700000e+00,  -2.08600000e+00,   1.38000000e-01],
       [  6.50000000e-02,   1.00000000e+00,   4.90000000e-01,
         -4.87600000e+00,  -1.25100000e+00,  -3.14000000e-01],
       [  3.00000000e-02,   1.90000000e-02,   1.00000000e+00,
          4.80000000e-02,   5.63000000e-01,  -2.65000000e-01],
       [  8.00000000e-03,  -1.60000000e-02,   4.00000000e-03,
          1.00000000e+00,  -9.30000000e-02,   3.70000000e-02],
       [ -1.60000000e-02,  -3.10000000e-02,   3.51000000e-01,
         -7.11000000e-01,   1.00000000e+00,  -1.64000000e-01],
       [  3.00000000e-03,  -2.10000000e-02,  -4.45000000e-01,
          7.57000000e-01,  -4.42000000e-01,   1.00000000e+00]])
>>> pd.DataFrame(C2).round(3)
       0      1      2      3      4      5
0  1.000  0.215  2.498  7.887 -2.086  0.138
1  0.065  1.000  0.490 -4.876 -1.251 -0.314
2  0.030  0.019  1.000  0.048  0.563 -0.265
3  0.008 -0.016  0.004  1.000 -0.093  0.037
4 -0.016 -0.031  0.351 -0.711  1.000 -0.164
5  0.003 -0.021 -0.445  0.757 -0.442  1.000
>>> hist
>>> hist -o -p
