diff --git a/manuscript/Appendix D -- Machine Learning.asc b/manuscript/Appendix D -- Machine Learning.asc
index eeb334e..986cd50 100644
--- a/manuscript/Appendix D -- Machine Learning.asc	
+++ b/manuscript/Appendix D -- Machine Learning.asc	
@@ -236,33 +236,36 @@ If you have an array of all your model classifications or predictions in numpy a
 ----
 >>> y_true = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])  # <1>
 >>> y_pred = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0, 0])  # <2>
->>> true_positives = ((y_pred == y_true) & (y_true == 1)).sum()
+>>> true_positives = ((y_pred == y_true) & (y_pred == 1)).sum()
 >>> true_positives  # <3>
 4
->>> true_negatives = ((y_pred == y_true) & (y_true == 0)).sum()
+>>> true_negatives = ((y_pred == y_true) & (y_pred == 0)).sum()
 >>> true_negatives  # <4>
 2
 ----
 <1> `y_true` is a numpy array of the true (correct) class labels. Usually these are determined by a human
 <2> `y_pred` is a numpy array of your model's predicted class labels (0 or 1)
-<3> `true_positives` are the positive class labels (1) that your model got right (labeled a 1 when they should be)
-<4> `true_negatives` are the negative class labels (0) that your model got right (labeled a 0 when they should be)
+<3> `true_positives` are the positive class labels (1) that your model got right (correctly labeled 1)
+<4> `true_negatives` are the negative class labels (0) that your model got right (correctly labeled 0)
 
-Often it's also important to count up the predictions that your model got wrong, like this:
+It's also important to count up the predictions that your model got wrong, like this:
 
 .Count what the model got wrong
 [source,python]
 ----
->>> false_positives = ((y_pred != y_true) & (y_true == 1)).sum()
+>>> false_positives = ((y_pred != y_true) & (y_pred == 1)).sum()
 >>> false_positives  # <1>
-3
->>> false_negatives = ((y_pred != y_true) & (y_true == 0)).sum()
->>> false_negatives  # <2>
 1
+>>> false_negatives = ((y_pred != y_true) & (y_pred == 0)).sum()
+>>> false_negatives  # <2>
+3
 ----
-<1> `true_positives` are the positive class labels (1) that your model got right (labeled a 1 when they should be)
-<2> `true_negatives` are the negative class labels (0) that your model got right (labeled a 0 when they should be)
+<1> `false_positives` are the negative class examples (1) that were falsely labeled positive by your model (labeled 1 when they should be 0)
+<2> `false_negatives` are the positive class examples (0) that were falsely labeled negative by your model (labeled 0 when they should be 1)
+
 
+// PROOFER: user [557658](https://forums.manning.com/posts/list/45137) found this bug and it's been fixed above
+ 
 Sometimes these four numbers are combined into a single 4x4 matrix called an error matrix or confusion matrix.
 Here's what our made-up predictions and truth values would look like in a confusion matrix:
 
@@ -279,8 +282,8 @@ Here's what our made-up predictions and truth values would look like in a confus
 >>> confusion
               1  0
 pred \ truth      
-1             4  3
-0             1  2
+1             4  1
+0             3  2
 ----
 
 In a confusion matrix, you want to have large numbers along the diagonal (upper left and lower right) and low numbers in the off diagonal (upper right and lower left).
